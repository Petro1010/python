How to reduce loss?

- We can iteratively reduce loss
- Start with a wild guess for w1 and b, and see what the loss is. Then try another guess and see what the loss is. Keep going until you find the best possible loss.
Example:
- We have a linear model, y'= b + w1x1, which we use as the predicition function
- We also must choose a loss function to compute our loss (square error)
- When the loss has been calculated, the system will come up with new w1 and b parameters that will hopefully reduce loss
- Convergence of model: Iterate until overall loss stops changing or changes are extremely small.

Gradient Descent (How we go about coming up with our new parameter predictions):
- Loss vs w1 will almost always be convex or bowl shaped
- Convex problems have only one minimum
- Step 1: pick a starting value for w1
- Step 2: Calculate the gradient of the loss, which is the deritvative (slope) of the curve (tells you to move left or right). When there are multiple weights, we must compute partial derivatives
	- Gradient is a vector, so it has direction and magnitude
- Step 3: take step in direction of negative gradient in order to reduce loss quickly

Learning Rate/Step Size:
- gradient descent multiply the gradient by the learning rate to identify the next point
- Hyperparameters: knobs that programmers tweak. The learning rate is one of these things
- A small learning rate may take forever to find the minimum
- A large learning rate may overshoot it
- Need to try to find the correct learning rate to get to the minimum in a small amount of time.

Types of Gradient Descent:
- Batch: total number of examples used to calcualte the gradient
- a very large batch may cause a single iteration to take forever to compute
	- redundauncy can tend to occur
- Stochastic gradient descent (SGD): uses only a single example
- Mini-batch Stochastic gradient descent (mini-batch SGD): uses 10-1000 random examples. More efficient and less noise

Hyperparameter tuning:
- Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.
- If the training loss does not converge, train for more epochs.
- If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.
- If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.
- Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.
- Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.
- For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory.

Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.
